# -*- coding: utf-8 -*-
"""PredictingUsedCarSalePriceUsingFeedforwardArtificalNeuralNetworks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tYTq4ueI7hRBgNKmi5ZUUacOvOVYehBA
"""

import tensorflow as tf
print(tf.__version__)
import seaborn as sns
import pandas as pd
import numpy as np
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

from google.colab import drive
drive.mount('/content/drive')

car_data = pd.read_csv('/content/drive/MyDrive/AIC 2024/AIC_Files/car_data.csv')

car_data.shape

car_data.head()

car_data.drop(columns=["New_Price", "Unnamed: 0", "Location", "Fuel_Type", "Transmission", "Owner_Type", "Mileage", "Engine", "Power"], inplace=True)

car_data.drop(columns=["Name"], inplace=True)
#makes the numerical and categorical categories
numerical = car_data.select_dtypes(include=[np.number]).columns.tolist()
categorical = car_data.select_dtypes(include=["object"]).columns.tolist()

#does the one-hot coding for the program
car_data = pd.get_dummies(car_data, columns=categorical)
car_data.head()

car_data.dropna(inplace=True)
#drops everything that is false

#divides into features and labels
X = car_data.drop(columns=["Price"])
y = car_data["Price"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)#training data

#standardizing the scale
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)

plt.hist(car_data["Price"])

print(type(car_data))

car_data.dropna(inplace=True)#drops anything deemd n/a

def create_model(learning_rate, dropout_rate):
    model = Sequential()
    model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(6, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    adam = Adam(learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
    return model
#creates a lot of nodes in order for the model to function as a neural network

dropout_rate = 0.1
epochs = 20
batch_size=4
learn_rate=0.001
#sets the parameters for the neural network to be working

model = create_model(learn_rate, dropout_rate)
#finishes creating the model through the previous function

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model_plot1.png', show_shapes=True, show_layer_names=True)
#imports prexisting libraries to import a model

input_layer = Input(shape=(X.shape[1],))
dense_layer0 = Dense(100, activation='relu')(input_layer)
dense_layer1 = Dense(50, activation='relu')(dense_layer0)
dense_layer2 = Dense(25, activation='relu')(dense_layer1)
dense_layer3 = Dense(10, activation='relu')(dense_layer2)
dense_layer4 = Dense(5, activation='relu')(dense_layer3)
dense_layer5 = Dense(2, activation='relu')(dense_layer4)
output = Dense(1)(dense_layer5)
#gives all of the nodes in order for it to be working

model = Model(inputs=input_layer, outputs=output)
model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mae'])

#complies everything

model_history = model.fit(X_train, y_train, validation_split=0.2, epochs=30, batch_size=5)
#removes overfitting

accurancies = model.evaluate(X_test, y_test, verbose=1)
print("Test Score:", accurancies[0])
print("Test Accurancy:", accurancies[1])
#tells accurancy

history_dict=model_history.history
print(history_dict.keys())
#tells teh values being stored

import matplotlib.pyplot as plt
plt.plot(model_history.history['loss'], label = 'val_loss')
plt.plot(model_history.history['loss'], label='val_loss')
plt.legend(['train', 'test'], loc = 'lower left')
#plots loss vs value loss

plt.plot(model_history.history['mae'], label='val_mae')
plt.plot(model_history.history['mae'], label='val_mae')
plt.legend(['train', 'test'], loc='upper left')
#plots mae vs value mae

y_pred = model.predict(X_test)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
#details the error for each of the situations

car_data["Price"].mean()#average price

comparison_df=pd.DataFrame({'Actual':y_test.values.tolist(), 'Predicted': y_pred.tolist()})
comparison_df
#gives a dataframe of actual vs predicted

single_point = X_test[1].reshape(1, -1)
single_point.shape
#reshapes the first point from the actual file

predicted_values = model.predict(X_test[1].reshape(1,-1))
#predicts the first point from the actual file

actual_test_values = y_test.values[1]
#actual values

print(predicted_values)
print(actual_test_values)