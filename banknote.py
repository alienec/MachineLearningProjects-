# -*- coding: utf-8 -*-
"""BankNote.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YtZgwbbM8DHGDvZYKm27enVhCtT9Wr2Y

In K Means clustering , K refers to the number of clusters that you want your
data to be grouped into.
In K Means clustering , the number of clusters must be defined before K
clustering can be applied to the data points.

Steps for K-Means Clustering
1. Randomly assign centroid values for each cluster.
2. Calculate the distance (Euclidean or Manhattan ) between each data
point and centroid values of all the clusters.
3. As sign the data point to the cluster of the centroid with the
shortest distance.
4. Calculate and update centroid values based on the mean values of
the coordinates of all the data points of the corresponding
cluster.
5. Repeat steps 2â€“4 until new centroid values for all the clusters are different from the previous centroid values.

Why use K-Means Clustering?

ADVANTAGES
1. K Means clustering is a simple
to implement algorithm
2. Can be applied to large datasets
3. Scales well to unseen data points
4. Generalizes well to clusters of
various sizes and shapes.

DISADVANTAGES
1. The value of K must be chosen
manually
2. Convergence or training time
depends on the initial value of K
3. Clustering performance is
affected greatly by outliers.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
# %matplotlib inline

# generating dummy data of 500 records with 4 clusters
bank = pd.read_csv('banknote (1).csv')
features, labels = make_blobs(n_samples=500, centers=4,
cluster_std = 2.00)
# plotting the dummy data
plt.scatter(features[:,0], features[:,1] )

# performing kmeans clustering using KMeans class
km_model = KMeans(n_clusters=4)
km_model.fit(features)

#converting categorial labels to number
from sklearn import preprocessing

le = preprocessing.LabelEncoder()
labels_num = le.fit_transform(labels)

from sklearn.metrics.cluster import adjusted_rand_score
# adjusted_rand_score(labels_true, labels_pred)
print("ARI = ", adjusted_rand_score(labels_num, km_model.labels_))

#printing centroid values
print(km_model.cluster_centers_)

print(km_model.labels_)

#print the data points
plt.scatter(features[:,0], features[:,1], c=km_model.labels_, cmap='rainbow')
#print the centroids
plt.scatter(km_model.cluster_centers_[:,0], km_model.cluster_centers_[:,1], s=100, color='black')

#print actual datapoints

plt.scatter(features[:,0], features[:,1], c=labels_num, cmap='rainbow')

import seaborn as sns
iris_df = sns.load_dataset('iris')
iris_df.head()

#dividing data into features and labels
features = bank.drop(['class'], axis=1)
labels = bank.filter(['class'], axis=1)
features.head()

#training KMeans model
features = features.values
km_model = KMeans(n_clusters=4)
km_model.fit(features)

print(km_model.labels_)

#print the data points
plt.scatter(features[:,0], features[:,1], c=km_model.labels_, cmap='rainbow')
#print the centroids
plt.scatter(km_model.cluster_centers_[:,0], km_model.cluster_centers_[:,1], s=100, color='black')

#converting categorical labels to numbers
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
labels_num = le.fit_transform(labels)

from sklearn.metrics.cluster import adjusted_rand_score

#adjusted_rand_score(labels_true, labels_pred)
print("ARI = ", adjusted_rand_score(labels_num, km_model.labels_))

#training KMeans on K values from 1 to 10
loss = []
for i in range(1, 11):
  km  = KMeans(n_clusters = i).fit(features)
  loss.append(km.inertia_)

#print loss against number of clusters

import matplotlib.pyplot as plt
plt.plot(range(1, 11), loss)
plt.title('Finding Optimal ')
plt.xlabel('Number of clusters')
plt.ylabel('Loss')
plt.show()

# training KMeans with 3 clusters
km_model = KMeans(n_clusters=3)
km_model.fit(features)

#print the data points with predicted labels

plt.scatter(features[:, 0], features[:,1], c=km_model.labels_, cmap='rainbow')

#print the predicted centroids

plt.scatter(km_model.cluster_centers_[:,0], km_model.cluster_centers_[:,1], s=100, color='black')

#converting categorical labels to numbers
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
labels_num = le.fit_transform(labels)

#print the data points with original labels
plt.scatter(features[:,0], features[:,1], c=labels_num, cmap='rainbow')

#converting categorical labels to numbers
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
labels_num = le.fit_transform(labels)

from sklearn.metrics.cluster import adjusted_rand_score
#adjusted_rand_score(labels_true, labels_pred)
print("ARI = ", adjusted_rand_score(labels_num, km_model.labels_))

"""Hierarchical clustering can be broadly divided into two types:
agglomerative clustering and divisive clustering
Agglomerative clustering follows a bottom-up approach, where individual
data points are clustered together to form multiple small clusters leading to
a big cluster, which can then be divided into small clusters using
dendrograms.
On the other hand, in the case of divisive clustering , you have one big
cluster, which you divide into N number of small clusters.

Steps for Hierarchical Agglomerative Clustering
1. Consider each data point in the dataset as one cluster. Hence, the
number of clusters in the beginning is equal to the number of
data points.
2. Join the two closest data points to form a cluster.
3. Form more clusters by joining the closest clusters. Repeat this
process until one big cluster is formed.
4. Use dendrograms to divide the one big cluster into multiple small
clusters. A dendrogram is a branching diagram that represents
the relationships of similarity among a group of entities.

Why use Hierarchial Clustering



---

Advantages


*   Unlike K Means clustering, for hierarchial clustering, you do not need to specify the number of centroids clustering.
*   With dendrograms, it is easier to interpret how the data has been clustered

Disadvantages

* Doesn't scale well with unseen data
* Higher time complexity compared to K Means clustering
* Difficult to determine the number of clusters in case of large dataset
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
from matplotlib import pyplot as plt
# %matplotlib inline

#generating dummy data of 10 records with 2 clusters
features, labels = make_blobs(n_samples=10, centers=2, cluster_std=2.00)

#plotting the dummy data
plt.scatter(features[:,0], features[:,1])
#adding numbers to data points
annots = range(1,11)
for label,x,y in zip(annots, features[:,0], features[:, 1]):
  plt.annotate(label,xy = (x,y) , xytext = (-3,3), textcoords = 'offset points', ha = 'right', va = 'bottom')
plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage
dendos = linkage(features, 'single')
annots = range(1,11)
dendrogram(dendos, orientation='top', labels=annots, distance_sort='descending', show_leaf_counts=True)
plt.show()

from sklearn.cluster import AgglomerativeClustering
hc_model = AgglomerativeClustering(n_clusters=2, metric='euclidean', linkage='ward')
hc_model.fit_predict(features)
plt.scatter(features[:,0], features[:,1], c=hc_model.labels_, cmap='rainbow')

features, labels = make_blobs(n_samples=500, centers=4,
cluster_std = 2.00)
# plotting the dummy data
plt.scatter(features[:,0], features[:,1] )

hc_model.fit_predict(features)
plt.scatter(features[:,0], features[:,1], c=hc_model.labels_, cmap='rainbow')

plt.scatter(features[:,0], features[:,1], c=labels, cmap='rainbow')

#converting numerical labels to numbers
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
labels_num = le.fit_transform(labels)

#when labels become numeric

from sklearn.metrics.cluster import adjusted_rand_score
#adjusted_rand_score(labels_true, labels_pred)
print("ARI = ", adjusted_rand_score(labels_num, hc_model.labels_))

import seaborn as sns
iris_df = sns.load_dataset('iris')
iris_df.head()

features  = bank.drop(['class'], axis=1)
labels = bank.filter(['class'], axis=1)
features.head()

from sklearn.cluster import AgglomerativeClustering
hc_model = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
hc_model.fit_predict(features)

print(hc_model.labels_)

features = features.values
plt.scatter(features[:,0], features[:, 1], c= hc_model.labels_, cmap='rainbow')

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
labels_num = le.fit_transform(labels)

from sklearn.metrics.cluster import adjusted_rand_score
#adjusted_rand_score(labels_true, labels_pred)
print("ARI = ", adjusted_rand_score(labels_num, hc_model.labels_))

import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))
plt.title("Iris Dendrogram")
dend = shc.dendrogram(shc.linkage(features, method='ward'))

plt.scatter(features[:,0], features[:,1], c=labels_num, cmap='rainbow')

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
labels_num = le.fit_transform(labels)

from sklearn.metrics.cluster import adjusted_rand_score
#adjusted_rand_score(labels_true, labels_pred)
print("ARI = ", adjusted_rand_score(labels_num, hc_model.labels_))

import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))
plt.title("Iris Dendrogram")
dend = shc.dendrogram(shc.linkage(features, method='ward'))

plt.scatter(features[:,0], features[:,1], c=labels_num, cmap='rainbow')