# -*- coding: utf-8 -*-
"""DimensionalityReduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eJcUdpo1QqEZSfeH7GSQf7q4sO18nflm

Dimensionality Reduction
* reducing the number of features in a dataset
* overall performance of algorithms in dataset minimally affected
* training time is reduced
* visualized more easily due to less data and lower dimensions
* Principal Component Analysis (PCA) & Linear Discriminant Analysis (LDA)

Principal Component Analysis (Unsupervised)

* doesn't depend on labels of a dataset
* goal is to maximize variance in output through prioritizing features
* purpose is to capture features that contain maximum features
* feature causing 1st maximum variance - first principal component, etc: second maximum variance - second principal component

Advantages

* correlated features can be detected and removed
* decreases overfitting due to reduction of features  
* model training can be scaled faster and be faster

Cons
* independent variable becomes less able to be integrated
* requires standardization of data
* small amount of information is lost decreasing accurancy

Implementing PCA with Sklearn
"""

import pandas as pd
import numpy as np
import seaborn as sns

iris_df = sns.load_dataset('iris')
iris_df.head()

# For PCA, only use the feature set

#following script divides it into features and labels

X = iris_df.drop(["species"], axis=1)
y = iris_df["species"]

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
y = le.fit_transform(y)

#divides the data into training and test sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#apply scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#import the PCA class (there is a class for Principal Component Analysis)
# pass training set of data into the method fit_transform()
#pass test set to transform() to apply PCA

#import PCA class
from sklearn.decomposition import PCA
#create class of PCA
pca = PCA()
#training PCA model on training data
X_train = pca.fit_transform(X_train)
#make predictions on test data
X_test = pca.transform(X_test)

#after applying PCA on dataset, use explained_variance_ratio_ to print variance based on all the features in the dataset


#print variance ratios
variance_ratios = pca.explained_variance_ratio_
print(variance_ratios)

"""The output on the top will give a result of 72.23% variance in the index of 0 indicating that it is caused by the first principal component while 23.97 found in the index of 1 is caused by the second principal component.

Creating and Training a Classification model
"""

#select two principal components causing a variance of 96.19% due to the first and second principal component
#to do this, pass 2 as value to n_components attribute of the PCA class
#selects two principal componenets from the Iris training and testing sets

from sklearn.decomposition import PCA
pca = PCA(n_components=2) #attribute of PCA given a specific value
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

"""The code on the bottom depicts the following:

* classification model with logistic regression
* predicts the label of iris plant with two prinicipal components/features
"""

#making predictions from logistic regression
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)
#training the logisitic regression model
lg = LogisticRegression()
lg.fit(X_train, y_train)
#predict the test set results
y_pred = lg.predict(X_test)
#evaluate result
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, y_pred))
#0.8(6 repeating)

"""The output tells that the accurancy for predicting the label for the iris plant is 86.66%."""

# Commented out IPython magic to ensure Python compatibility.
#easy visualization

from matplotlib import pyplot as plt
# %matplotlib inline
#print actual datapoints
plt.scatter(X_test[:, 0], X_test[:, 1], c= y_test, cmap="rainbow")

"""Linear Discriminant Analysis (Supervised)

* decision boundary is built around data points between clusters belonging to each cluster in a class
* data points projected into new dimensions in way which distance between data points in a cluster is minimized while distance between clusters is maximized
* new dimensions ranked on their ability to
    (i) minimize distance between points in a cluster
    (ii) maximize distance between individual clusters

Advantages
* reduces overfitting due to reduction of features (like PCA)
* model training can be expedited (like PCA)

Disadvantages
* not able to detect correlated features
* cannot be used with unsupervised/unlabeled data
* amount of information is lost when reduced features (PCA)
"""

banknote_df = pd.read_csv("banknote.csv")
banknote_df.head()

#dividing the data into features and labels
X = banknote_df.drop(["class"], axis=1)
y = banknote_df.filter(["class"], axis=1)

#divide the data into training data and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
#scale the data using StandardScaler()
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""To apply LDA to sklearn do...

* LinearDiscriminantAnalyis from Sklearn.decomposition module
* pass training set through fit.transform() method of the LDA module
* pass the test set to the transform() method of LDA class project
* similar to the PCA module: import, fit.transform(), transform()

"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
#create the object of the LDA class
lda = LDA()
#training LDA model on training data
X_train = lda.fit_transform(X_train, y_train)
#making prediction based on test data
X_test = lda.transform(X_test)

"""---Can use explained_variance_ratio_ to figure out the variance ratios for LDA like PCA"""

variance_ratios = lda.explained_variance_ratio_
print(variance_ratios)

"""----Shows how even with one componenet, maximum variance can be reached"""

#there is a n_componenets attribute for both the LDA and PCA class
lda = LDA(n_components=1)
X_train = lda.fit_transform(X_train, y_train)
X_test = lda.transform(X_test)

#then we use PCA for figuring out whether or not predicting the values in the banknote are fake or not using a single feature
#use LogisticRegression() algorithm
lg = LogisticRegression()
lg.fit(X_train, y_train)
y_pred = lg.predict(X_test)
print(accuracy_score(y_test, y_pred))

"""- Able to predict whether or not the values in the banknote data is fake or not with 98.9% accurancy

### Implementing PCA with the customer_churn.csv and print the accurancy using two principal components and using matplotlib, plot the results on the test using two principal components
"""

customer = pd.read_csv("customer_churn (1).csv")
customer.head()

X = customer.drop( ["RowNumber", "Surname", "Geography", "Gender", "Exited"], axis=1)
y = customer["Exited"]

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

pca = PCA()
#training PCA model on training data
X_train = pca.fit_transform(X_train)
#make predictions on test data
X_test = pca.transform(X_test)

from sklearn.decomposition import PCA
pca = PCA(n_components=2) #attribute of PCA given a specific value
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)
variance_ratios = pca.explained_variance_ratio_
print(variance_ratios)

classifier = LogisticRegression()
classifier.fit(X_train, y_train)
#training the logisitic regression model
lg = LogisticRegression()
lg.fit(X_train, y_train)
#predict the test set results
y_pred = lg.predict(X_test)
#evaluate result
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, y_pred))

"""It can predict whether or not the data in the customer_churn.csv is fake or not with an accurancy of 79.75%."""

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot as plt
# %matplotlib inline
#print actual datapoints
plt.scatter(X_test[:, 0], X_test[:, 1], c= y_test, cmap="rainbow")