# -*- coding: utf-8 -*-
"""Sentiment Analysis for Various Reviews Using Na√Øve Bayes Algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FsPpE-lWg0Ko07O1UAjFQAJv-tusHbtf

### amazon_cells_labeled_txt
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.naive_bayes import MultinomialNB
from wordcloud import WordCloud
# %matplotlib inline

amazon = pd.read_csv('amazon_cells_labelled.txt', sep='\t', header=None)
amazon.columns = ["text", "sentiment"]
amazon.head()

amazon.shape

amazon.sentiment.value_counts().plot(kind='barh', figsize = (8,4))

from nltk.corpus import stopwords
nltk.download("stopwords")
from textwrap import fill
print(fill(', '.join(stopwords.words("english")), width = 80))
stop = stopwords.words("english")
amazon['text'] = amazon['text'].apply(lambda x: ' '. join([item for item in x.split() if item not in stop]))

amazon_spam = amazon[amazon["sentiment"] == 1]
plt.rcParams["figure.figsize"] = [8, 10 ]
text = ' '.join(amazon_spam["text"])
wordcloud2 = WordCloud().generate(text)
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

amazon_ham = amazon[amazon["sentiment"] == 0]
plt.rcParams["figure.figsize"] = [8, 10]
text = ' '.join(amazon_ham["text"])
wordcloud2 = WordCloud().generate(text)
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

X = amazon["text"]
y = amazon["sentiment"]

def clean_text(doc):
  document = re.sub('[^a-zA-Z]', ' ', doc)
  document = re.sub(r"\s+[a-zA-z]\s+", ' ', document)
  document = re.sub(r"\s+", ' ', document)
  return document

X_sentences = []
reviews = list(X)
for rev in reviews:
  X_sentences.append(clean_text(rev))

from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=2500, min_df=5, max_df=0.7, stop_words = stopwords.words('english'))
X = vectorizer.fit_transform(X_sentences).toarray()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

spam_detector = MultinomialNB()
spam_detector.fit(X_train, y_train)

y_pred = spam_detector.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

print(X_sentences[56])
print(y[56])

"""### imdb.labelled.txt"""

imdb = pd.read_csv('imdb_labelled.txt', sep='\t', header=None)
imdb.columns = ["text", "sentiment"]
imdb.head()

imdb.shape

imdb.sentiment.value_counts().plot(kind='barh', figsize = (8,4))

from nltk.corpus import stopwords
nltk.download("stopwords")
from textwrap import fill
print(fill(', '.join(stopwords.words("english")), width = 80))
stop = stopwords.words("english")
imdb['text'] = imdb['text'].apply(lambda x: ' '. join([item for item in x.split() if item not in stop]))

imdb_spam = imdb[imdb["sentiment"] == 1]
plt.rcParams["figure.figsize"] = [8, 10 ]
text = ' '.join(imdb_spam["text"])
wordcloud2 = WordCloud().generate(text)
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

imdb_ham = imdb[imdb["sentiment"] == 0]
plt.rcParams["figure.figsize"] = [8, 10 ]
text = ' '.join(imdb_ham["text"])
wordcloud2 = WordCloud().generate(text)
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

X = imdb["text"]
y = imdb["sentiment"]

def clean_text(doc):
  document = re.sub('[^a-zA-Z]', ' ', doc)
  document = re.sub(r"\s+[a-zA-z]\s+", ' ', document)
  document = re.sub(r"\s+", ' ', document)
  return document

X_sentences = []
reviews = list(X)
for rev in reviews:
  X_sentences.append(clean_text(rev))

from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=2500, min_df=5, max_df=0.7, stop_words = stopwords.words('english'))
X = vectorizer.fit_transform(X_sentences).toarray()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

spam_detector = MultinomialNB()
spam_detector.fit(X_train, y_train)

y_pred = spam_detector.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

print(X_sentences[56])
print(y[56])

"""### yelp_labelled_txt"""

yelp = pd.read_csv('yelp_labelled.txt', sep='\t', header=None)
yelp.columns = ["text", "sentiment"]
yelp.head()

yelp.shape

yelp.sentiment.value_counts().plot(kind='barh', figsize = (8,4))

from nltk.corpus import stopwords
nltk.download("stopwords")
from textwrap import fill
print(fill(', '.join(stopwords.words("english")), width = 80))
stop = stopwords.words("english")
yelp['text'] = yelp['text'].apply(lambda x: ' '. join([item for item in x.split() if item not in stop]))

yelp_spam = yelp[yelp["sentiment"] == 1]
plt.rcParams["figure.figsize"] = [8, 10 ]
text = ' '.join(yelp_spam["text"])
wordcloud2 = WordCloud().generate(text)
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

yelp_ham = yelp[yelp["sentiment"] == 0]
plt.rcParams["figure.figsize"] = [8, 10 ]
text = ' '.join(yelp_ham["text"])
wordcloud2 = WordCloud().generate(text)
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

X = yelp["text"]
y = yelp["sentiment"]

def clean_text(doc):
  document = re.sub('[^a-zA-Z]', ' ', doc)
  document = re.sub(r"\s+[a-zA-z]\s+", ' ', document)
  document = re.sub(r"\s+", ' ', document)
  return document

X_sentences = []
reviews = list(X)
for rev in reviews:
  X_sentences.append(clean_text(rev))

from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=2500, min_df=5, max_df=0.7, stop_words = stopwords.words('english'))
X = vectorizer.fit_transform(X_sentences).toarray()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

spam_detector = MultinomialNB()
spam_detector.fit(X_train, y_train)

y_pred = spam_detector.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

print(X_sentences[56])
print(y[56])