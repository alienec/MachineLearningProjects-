# -*- coding: utf-8 -*-
"""RNN notebook

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bt-PSeS0racH72J7NkFwk2alfjN7oDIn
"""

# Importing libraries
import pandas as pd
import numpy as np

# Importing dataset (use the path to your Google Drive)
fb_complete_data = pd.read_csv("fb_train.csv")

# Printing dataset header
fb_complete_data.head()

# Filtering 'Open' column
fb_training_processed = fb_complete_data[['Open']].values

# Next, we will scale our dataset:

# Scaling features
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
fb_training_scaled = scaler.fit_transform(fb_training_processed)

# Checking the length of the scaled dataset
len(fb_training_scaled)

# Training features contain data of the last 60 days
# Training labels contain data of the 61st day
fb_training_features = []
fb_training_labels = []

for i in range(60, len(fb_training_scaled)):
    fb_training_features.append(fb_training_scaled[i - 60:i, 0])
    fb_training_labels.append(fb_training_scaled[i, 0])

# Converting training data to numpy arrays
X_train = np.array(fb_training_features)
y_train = np.array(fb_training_labels)

# Let's print the shape of our dataset:
print(X_train.shape)
print(y_train.shape)
# Output should be (1197, 60) and (1197,)

# We need to reshape our input features into 3-dimensional format:
# Converting data into 3D shape
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

# Importing libraries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Activation, Dense, Flatten, Dropout, LSTM
from tensorflow.keras.models import Model

# Defining the LSTM network
input_layer = Input(shape=(X_train.shape[1], 1))

lstm1 = LSTM(100, activation='relu', return_sequences=True)(input_layer)
do1 = Dropout(0.2)(lstm1)

lstm2 = LSTM(100, activation='relu', return_sequences=True)(do1)
do2 = Dropout(0.2)(lstm2)

lstm3 = LSTM(100, activation='relu', return_sequences=True)(do2)
do3 = Dropout(0.2)(lstm3)

lstm4 = LSTM(100, activation='relu')(do3)
do4 = Dropout(0.2)(lstm4)

output_layer = Dense(1)(do4)

# Creating the model
model = Model(input_layer, output_layer)

# Compiling the model
model.compile(optimizer='adam', loss='mse')

# Plotting model architecture (use your path!)
from tensorflow.keras.utils import plot_model

plot_model(model)

# Printing the shape of the training data
print(X_train.shape)
print(y_train.shape)

# Reshaping y_train
y_train = y_train.reshape(-1, 1)

# Printing the new shape of y_train
print(y_train.shape)
# Output:
# (1197, 60, 1)
# (1197,)
# (1197, 1)

# The following script trains our stock price prediction model on the training set:
# Training the model
model_history = model.fit(X_train, y_train, epochs=100, verbose=1, batch_size=32)

# Creating test set (use your path!!!)
fb_testing_complete_data = pd.read_csv("fb_test.csv")

fb_testing_processed = fb_testing_complete_data[['Open']].values

# Let's concatenate the training and test sets. We do this to predict the first value in the test set.
# The input will be the data from the past 60 days, which is basically the data from the last 60 days in the training set.
fb_all_data = pd.concat((fb_complete_data['Open'], fb_testing_complete_data['Open']), axis=0)

# Creating the final input feature set
test_inputs = fb_all_data[len(fb_all_data) - len(fb_testing_complete_data) - 60:].values
print(test_inputs.shape)
# (80, )

# Testing the Stock Prediction Model
# You can see that the length of the input data is 80. Here, the first 60 records are the last 60 records from the training data, and the last 20 records are the 20 records from the test file.

# We need to scale our data and convert it into a column vector.
test_inputs = test_inputs.reshape(-1, 1)
test_inputs = scaler.transform(test_inputs)
print(test_inputs.shape)
# (80, 1)

# As we did with the training data, we need to divide our input data into features and labels.
fb_test_features = []

for i in range(60, 80):
    fb_test_features.append(test_inputs[i - 60:i, 0])

# Testing the Stock Prediction Model
# Let's now print our feature set.
X_test = np.array(fb_test_features)
print(X_test.shape)
# (20, 60)

# Our feature set is currently 2-dimensional. But the LSTM algorithm in Keras accepts only data in 3-dimensional format.
# The following script converts our input features into a 3-dimensional shape.
# Converting test data into 3D shape
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
print(X_test.shape)
# (20, 60, 1)

# Now is the time to make predictions on the test set.
# Making predictions on test set
y_pred = model.predict(X_test)
print(y_pred.shape)

# Since we scaled our input feature, we need to apply the inverse_transform() method of the scaler object on the predicted output to get the original output values.
# Converting scaled data back to original data
y_pred = scaler.inverse_transform(y_pred)

# Plotting the Stock Prediction Results
# Finally, to compare the predicted output with the actual stock price values, you can plot the two values:
plt.figure(figsize=(8, 6))
plt.plot(fb_testing_processed, color='red', label='Actual Facebook Stock Price')
plt.plot(y_pred, color='green', label='Predicted Facebook Stock Price')
plt.title('Facebook Stock Prices')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.show()